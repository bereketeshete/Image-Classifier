# Author Bereket Kebede, Graduate Student
# Neural Networks -  Assignment #2 - University of Memphis. Fall 2021
# Question #2, Designing a Convolutional Neural Network (CNN)
# Last updated - Sept 23, 2021


import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
import cv2
import os
#from tqdm import tqdm_notebook as tqdm  # Jupyter notebook should use this

os.environ['CUDA_VISIBLE_DEVICES'] = '0'

DATASET_PATH = '../input/15-scene/15-Scene/'

one_hot_lookup = np.eye(15)  # 15 classes

dataset_x = []
dataset_y = []

for category in sorted(os.listdir(DATASET_PATH)):
    print('loading category: ' + str(int(category)))
    for fname in os.listdir(DATASET_PATH + category):
        img = cv2.imread(DATASET_PATH + category + '/' + fname, 2)
        img = cv2.resize(img, (224, 224))
        dataset_x.append(np.reshape(img, [224, 224, 1]))
        dataset_y.append(np.reshape(one_hot_lookup[int(category)], [15]))

dataset_x = np.array(dataset_x)
dataset_y = np.array(dataset_y)

"""shuffle dataset"""
p = np.random.permutation(len(dataset_x))
dataset_x = dataset_x[p]
dataset_y = dataset_y[p]

test_x = dataset_x[:int(len(dataset_x) / 10)]
test_y = dataset_y[:int(len(dataset_x) / 10)]
train_x = dataset_x[int(len(dataset_x) / 10):]
train_y = dataset_y[int(len(dataset_x) / 10):]

print(train_x.shape)
print(train_y.shape)
print(test_x.shape)
print(test_y.shape)

"""fully connected layer"""


def dense(input_tensor, input_units, units, name_scope):
    with tf.name_scope(name_scope):
        W = tf.Variable(tf.random_normal((input_units, units)))
        b = tf.Variable(tf.random_normal((units,)))
        return tf.nn.relu(tf.matmul(input_tensor, W) + b)


"""convolution layer in 2D"""


def conv2d(input_tensor, filter_size, in_channels, out_channels, name_scope, activation=True):
    with tf.name_scope(name_scope):
        kernel = tf.Variable(tf.truncated_normal([filter_size, filter_size, in_channels, out_channels],
                                                 dtype=tf.float32, stddev=1e-1), name='weights', trainable=True)
        conv = tf.nn.conv2d(input_tensor, kernel, [1, 1, 1, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[out_channels], dtype=tf.float32), trainable=True, name='biases')
        conv_biased = tf.nn.bias_add(conv, biases)
        if activation == False:
            return conv_biased  # no ReLU activation
        relu_conv = tf.nn.relu(conv_biased)
        return relu_conv


"""build the CNN"""
X = tf.placeholder(tf.float32, [None, 224, 224, 1])
Y = tf.placeholder(tf.float32, [None, 15])

conv1 = conv2d(input_tensor=X, filter_size=3, in_channels=1, out_channels=16, name_scope='conv1')  # output [224,224,16]
pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  # output [112,112,16]
conv2 = conv2d(input_tensor=pool1, filter_size=3, in_channels=16, out_channels=32, name_scope='conv2')
pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  # output [56,56,32]
conv3 = conv2d(input_tensor=pool2, filter_size=3, in_channels=32, out_channels=32, name_scope='conv3')
pool3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  #
conv4 = conv2d(input_tensor=pool3, filter_size=3, in_channels=32, out_channels=32, name_scope='conv4')
pool4 = tf.nn.max_pool(conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  #

flatten = tf.reshape(pool4, [-1, 14 * 14 * 32])

fc1 = tf.layers.dense(flatten, 256, activation=tf.nn.relu)
fc2 = tf.layers.dense(fc1, 128, activation=tf.nn.relu)

output = tf.layers.dense(fc2, 15, activation=None)

loss = tf.losses.softmax_cross_entropy(onehot_labels=Y, logits=output)
loss = tf.reduce_mean(loss)

optimizer = tf.train.AdamOptimizer(0.00001).minimize(loss)
accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y, 1), tf.argmax(output, 1)), dtype=tf.float32))

sess = tf.Session()
init = tf.global_variables_initializer()
sess.run(init)

BATCH_SIZE = 16
epoch = 0
while epoch < 20:
    total_steps = int(len(train_x) / BATCH_SIZE)
    loss_value = 0
    for step in tqdm(range(total_steps), desc=('Epoch ' + str(epoch))):
        """get training batch"""
        if step * BATCH_SIZE + BATCH_SIZE < len(train_x):
            BATCH_X = train_x[step * BATCH_SIZE: step * BATCH_SIZE + BATCH_SIZE]
            BATCH_Y = train_y[step * BATCH_SIZE: step * BATCH_SIZE + BATCH_SIZE]
        else:
            BATCH_X = train_x[step * BATCH_SIZE:]
            BATCH_Y = train_y[step * BATCH_SIZE:]

        """train"""
        [loss_value, accuracy_value, _] = sess.run([loss, accuracy, optimizer], feed_dict={X: BATCH_X, Y: BATCH_Y})

    """end of epoch"""
    print('Epoch loss = ', loss_value, ' training accuracy = ', accuracy_value)
    test_acc = sess.run(accuracy, feed_dict={X: test_x[:100], Y: test_y[:100]})
    print('testing accuracy = ', test_acc)

    epoch += 1